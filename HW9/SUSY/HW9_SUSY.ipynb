{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 计算物理B第七次作业--第二题\n","\n","PB20511896 王金鑫"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 题目描述\n","\n","用深度神经网络求解寻找超对称问题\n","\n","- 使用单层隐藏神经元1000个，研究预言正确率与训练样本大小的关系，训练样本数目范围1000~4500000，画出关系图；\n","- 固定隐藏层神经元每层100个，研究正确率与隐藏层数的关系，层数范围1-5，画出关系图。"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["参考：https://physics.bu.edu/~pankajm/ML-Notebooks/HTML/NB13_CIX-DNN_susy_Pytorch.html\n","\n","数据集来源：https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n","\n","> 数据由MC方法生成。其中每个粒子有18个物理量特征，包括10个高级特征和8个低级特征，以及是否为超对称粒子的标签。\n","> "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 设置随机数种子\n","\n","这里使用当前时间作为随机数种子"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2798,"status":"ok","timestamp":1672805506660,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"d7rmJafiK2jK","outputId":"317d125e-7d2e-406d-80c1-363c1f254591"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x1591ce07790>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# from __future__ import print_function, division\n","import time\n","import numpy as np\n","import torch  # pytorch package, allows using GPUs\n","# set seed\n","seed = int(time.time())\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 自定义数据集\n","\n","定义一个SUSY数据集类，继承于`torch.utils.data.Dataset`\n","\n","- 将数据文件中的部分数据扔到数据集中，并划分为训练集和测试集\n","- 这里将0.8的数据划为训练集，0.2的划为测试集\n","- 可以选择只使用高级特征、低级特征，或同时使用两种特征\n","- 这里也给出了特征标签的列表"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1672805507150,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"BJnqVBlbL1Gz"},"outputs":[],"source":["from torchvision import datasets # load data\n","\n","class SUSY_Dataset(torch.utils.data.Dataset):\n","    \"\"\"SUSY pytorch dataset.\"\"\"\n","\n","    def __init__(self, data_file, root_dir, dataset_size, train=True, transform=None, high_level_feats=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            train (bool, optional): If set to `True` load training data.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","            high_level_festures (bool, optional): If set to `True`, working with high-level features only. \n","                                        If set to `False`, working with low-level features only.\n","                                        Default is `None`: working with all features\n","        \"\"\"\n","\n","        import pandas as pd\n","\n","        features=['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n","                'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2', \n","                'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n","        low_features=['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n","                'missing energy magnitude', 'missing energy phi']\n","        high_features=['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n","\n","\n","        #Number of datapoints to work with\n","        df = pd.read_csv(root_dir+data_file, header=None,nrows=dataset_size,engine='python')\n","        df.columns=features\n","        Y = df['SUSY']\n","        X = df[[col for col in df.columns if col!=\"SUSY\"]]\n","\n","        # set training and test data size\n","        train_size=int(0.8*dataset_size)\n","        self.train=train\n","\n","        if self.train:\n","            X=X[:train_size]\n","            Y=Y[:train_size]\n","            print(\"Training on {} examples\".format(train_size))\n","        else:\n","            X=X[train_size:]\n","            Y=Y[train_size:]\n","            print(\"Testing on {} examples\".format(dataset_size-train_size))\n","\n","\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        # make datasets using only the 8 low-level features and 10 high-level features\n","        if high_level_feats is None:\n","            self.data=(X.values.astype(np.float32),Y.values.astype(int))\n","            print(\"Using both high and low level features\")\n","        elif high_level_feats is True:\n","            self.data=(X[high_features].values.astype(np.float32),Y.values.astype(int))\n","            print(\"Using both high-level features only.\")\n","        elif high_level_feats is False:\n","            self.data=(X[low_features].values.astype(np.float32),Y.values.astype(int))\n","            print(\"Using both low-level features only.\")\n","\n","\n","\n","    # override __len__ and __getitem__ of the Dataset() class\n","\n","    def __len__(self):\n","        return len(self.data[1])\n","\n","    def __getitem__(self, idx):\n","\n","        sample=(self.data[0][idx,...],self.data[1][idx])\n","\n","        if self.transform:\n","            sample=self.transform(sample)\n","\n","        return sample\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 定义加载数据的函数\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672805507150,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"BYDWG3dbL3ar"},"outputs":[],"source":["def load_data(args):\n","\n","    data_file = 'SUSY.csv'\n","    # root_dir = os.path.expanduser('~')+'/ML_review/SUSY_data/'\n","    root_dir = './'\n","\n","    kwargs = {}  # CUDA arguments, if enabled\n","    if args.use_cuda:\n","        device = torch.device(\"cuda\")\n","\n","        kwargs = {'num_workers': 1,\n","                       'pin_memory': True}\n","\n","    # load and noralise train and test data\n","    train_loader = torch.utils.data.DataLoader(\n","        SUSY_Dataset(data_file, root_dir, args.dataset_size,\n","                     train=True, high_level_feats=args.high_level_feats),\n","        batch_size=args.batch_size, shuffle=True, **kwargs)\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        SUSY_Dataset(data_file, root_dir, args.dataset_size,\n","                     train=False, high_level_feats=args.high_level_feats),\n","        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n","\n","    return train_loader, test_loader\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 定义神经网络\n","\n","- 当 `single_layer==True` 时，求解第一小题，即使用单个含1000神经元的隐藏层\n","\n","- 当 `single_layer==False` 时，求解第二小题，即使用多个含100神经元的隐藏层，上限为5"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":560,"status":"ok","timestamp":1672805507705,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"ebLx7Ah_L5fG"},"outputs":[],"source":["import torch.nn as nn # construct NN\n","# implements forward and backward definitions of an autograd operation\n","import torch.nn.functional as F\n","\n","class model(nn.Module):\n","    def __init__(self,layers,single_layer=False,high_level_feats=None):\n","        # inherit attributes and methods of nn.Module\n","        super(model, self).__init__()\n","\n","        self.layers = layers\n","        self.single_layer = single_layer\n","\n","        # an affine operation: y = Wx + b\n","        if single_layer==True:\n","            if high_level_feats is None:\n","                self.fc1 = nn.Linear(18, 1000) # all features\n","            elif high_level_feats:\n","                self.fc1 = nn.Linear(10, 1000) # high-level only\n","            else:\n","                self.fc1 = nn.Linear(8, 1000) # low-level only\n","                \n","            self.batchnorm1 = nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1)\n","            self.fc2 = nn.Linear(1000, 2)\n","        else:\n","            if high_level_feats is None:\n","                self.fc1 = nn.Linear(18, 100)  # all features\n","            elif high_level_feats:\n","                self.fc1 = nn.Linear(10, 100) # high-level only\n","            else:\n","                self.fc1 = nn.Linear(8, 100) # low-level only\n","\n","            self.batchnorm1 = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n","            self.batchnorm2 = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n","            self.batchnorm3 = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n","            self.batchnorm4 = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n","            self.batchnorm5 = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1)\n","\n","            self.fc2 = nn.Linear(100, 100)\n","            self.fc3 = nn.Linear(100, 100)\n","            self.fc4 = nn.Linear(100, 100)\n","            self.fc5 = nn.Linear(100, 100)\n","            self.fc6 = nn.Linear(100, 2)\n","\n","\n","    def forward(self, x):\n","        '''Defines the feed-forward function for the NN.\n","\n","        A backward function is automatically defined using `torch.autograd`\n","\n","        Parameters\n","        ----------\n","        x : autograd.Tensor\n","            input data\n","\n","        Returns\n","        -------\n","        autograd.Tensor\n","            output layer of NN\n","\n","        '''\n","\n","        if self.single_layer==True:\n","            # apply rectified linear unit\n","            x = F.relu(self.fc1(x))\n","            # apply dropout\n","            x=self.batchnorm1(x)\n","            x = F.dropout(x, training=self.training)\n","\n","            # apply affine operation fc2\n","            x = self.fc2(x)\n","            # soft-max layer\n","            x = F.log_softmax(x, dim=1)\n","        else:\n","            x = F.relu(self.fc1(x))\n","            x=self.batchnorm1(x)\n","            x = F.dropout(x, training=self.training)\n","            if 1==self.layers:\n","                x = self.fc6(x)\n","                x = F.log_softmax(x, dim=1)\n","                return x\n","\n","            x = F.relu(self.fc2(x))\n","            x=self.batchnorm2(x)\n","            x = F.dropout(x, training=self.training)\n","            if 2==self.layers:\n","                x = self.fc6(x)\n","                x = F.log_softmax(x, dim=1)\n","                return x\n","\n","            x = F.relu(self.fc3(x))\n","            x = self.batchnorm1(x)\n","            x = F.dropout(x, training=self.training)\n","            if 3==self.layers:\n","                x = self.fc6(x)\n","                x = F.log_softmax(x, dim=1)\n","                return x\n","\n","            x = F.relu(self.fc4(x))\n","            x = self.batchnorm2(x)\n","            x = F.dropout(x, training=self.training)\n","            if 4==self.layers:\n","                x = self.fc6(x)\n","                x = F.log_softmax(x, dim=1)\n","                return x\n","\n","            x = F.relu(self.fc5(x))\n","            x = self.batchnorm1(x)\n","            x = F.dropout(x, training=self.training)\n","            x = self.fc6(x)\n","            x = F.log_softmax(x,dim=1)\n","            \n","        return x\n","        \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 定义模型评估函数\n","\n","将训练和测试整合，最后输出模型的测试结果"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1672805507706,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"kild2is3L8BR"},"outputs":[],"source":["import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n","\n","def evaluate_model(args,train_loader,test_loader):\n","\n","    # create model\n","    DNN = model(layers=args.layers,single_layer=args.single_layer,high_level_feats=args.high_level_feats)\n","    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n","    criterion = F.nll_loss\n","    # define SGD optimizer\n","    optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n","    #optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n","    if args.use_cuda:\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","\n","    ################################################\n","\n","    def train(epoch):\n","        '''Trains a NN using minibatches.\n","\n","        Parameters\n","        ----------\n","        epoch : int\n","            Training epoch number.\n","\n","        '''\n","\n","        # set model to training mode (affects Dropout and BatchNorm)\n","        DNN.train()\n","        # loop over training data\n","        for batch_idx, (data, label) in enumerate(train_loader):\n","            # zero gradient buffers\n","            optimizer.zero_grad()\n","            # compute output of final layer: forward step\n","            output = DNN(data)\n","            # compute loss\n","            loss = criterion(output, label.long())\n","            # run backprop: backward step\n","            loss.backward()\n","            # update weigths of NN\n","            optimizer.step()\n","            \n","            # print loss at current epoch\n","            if batch_idx % args.log_interval == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx * len(data), len(train_loader.dataset),\n","                    100. * batch_idx / len(train_loader), loss.item() ))\n","            \n","\n","        return loss.item()\n","\n","    ################################################\n","\n","    def test():\n","        '''Tests NN performance.\n","\n","        '''\n","\n","        # evaluate model\n","        DNN.eval()\n","\n","        test_loss = 0 # loss function on test data\n","        correct = 0 # number of correct predictions\n","        # loop over test data\n","        for data, label in test_loader:\n","            # compute model prediction softmax probability\n","            output = DNN(data)\n","            # compute test loss\n","            test_loss += criterion(output, label.long(), size_average=False).item() # sum up batch loss\n","            # find most likely prediction\n","            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","            # update number of correct predictions\n","            correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n","\n","        # print test loss\n","        test_loss /= len(test_loader.dataset)\n","        \n","        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)))\n","        \n","\n","        return test_loss, correct / len(test_loader.dataset)\n","\n","\n","    ################################################\n","\n","\n","    train_loss=np.zeros((args.epochs,))\n","    test_loss=np.zeros_like(train_loss)\n","    test_accuracy=np.zeros_like(train_loss)\n","\n","    epochs=range(1, args.epochs + 1)\n","    for epoch in epochs:\n","\n","        train_loss[epoch-1] = train(epoch)\n","        test_loss[epoch-1], test_accuracy[epoch-1] = test()\n","\n","\n","\n","    return test_loss[-1], test_accuracy[-1]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 定义结果绘图函数\n","\n","两个函数分别用图像将两个小题的结果展示出来"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672805507706,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"s-XWQgYBL-_v"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_data1(x,y):\n","\n","    # plot results\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    ax.plot(x,y)\n","    ax.set_xlabel('data sizes')\n","    ax.set_ylabel('accuracy')\n","    plt.show()\n","\n","\n","def plot_data2(x, y):\n","\n","    # plot results\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    ax.plot(x, y)\n","    ax.set_xlabel('number of layers')\n","    ax.set_ylabel('accuracy')\n","    plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 定义SGD训练函数\n","\n","使用随机梯度下降法（SGD）来训练模型\n","\n","下面两个函数分别对应两个小题\n","\n","- 将1000~4500000等间隔地划分为5个作为第一小题的数据集大小\n","- 第二小题的数据集大小固定为200000"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672805507706,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"_t0gEi7eMAxK"},"outputs":[],"source":["def grid_search1(args):\n","\n","    dataset_sizes = np.linspace(1000, 4500000, 5, dtype=int)\n","\n","    # pre-alocate data\n","    test_loss = np.zeros(len(dataset_sizes), dtype=np.float64)\n","    test_accuracy = np.zeros_like(test_loss)\n","\n","    # do grid search\n","    for i, dataset_size in enumerate(dataset_sizes):\n","        # upate data set size parameters\n","        args.dataset_size = dataset_size\n","        args.batch_size = int(0.01*dataset_size)\n","\n","        # load data\n","        train_loader, test_loader = load_data(args)\n","\n","        print(\"\\n training DNN with %d data points. \\n\" %dataset_size)\n","\n","        test_loss[i], test_accuracy[i] = evaluate_model(\n","            args, train_loader, test_loader)\n","\n","    plot_data1(dataset_sizes, test_accuracy)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672805507706,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"MEVgsbiHMDS7"},"outputs":[],"source":["def grid_search2(args):\n","\n","    layers_nums = np.linspace(1, 5, 5)\n","\n","    # pre-alocate data\n","    test_loss = np.zeros(len(layers_nums), dtype=np.float64)\n","    test_accuracy = np.zeros_like(test_loss)\n","\n","    # do grid search\n","    # upate data set size parameters\n","    args.dataset_size = 200000\n","    args.batch_size = int(2000)\n","    # load data\n","    train_loader, test_loader = load_data(args)\n","\n","    for i, layers_num in enumerate(layers_nums):\n","        args.layers = layers_num\n","        print(\"\\n training DNN with %d layers. \\n\" %layers_num)\n","\n","        test_loss[i], test_accuracy[i] = evaluate_model(\n","            args, train_loader, test_loader)\n","\n","    plot_data2(layers_nums, test_accuracy)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 进行训练\n","\n","使用 `argparse` 整合输入参数，使该程序可以通过命令行来方便地个性化参数\n","\n","其中默认的学习率为0.1"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672805507707,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"7xDMff8MMEE0","outputId":"239b4ff2-02ce-4d50-8ae3-e724a65ce618"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['--use_cuda'], dest='use_cuda', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='use cuda or cpu', metavar='HLF')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import argparse  # handles arguments\n","import sys\n","sys.argv = ['']\n","del sys  # required to use parser in jupyter notebooks\n","\n","# Training settings\n","parser = argparse.ArgumentParser(description='PyTorch SUSY Example')\n","parser.add_argument('--dataset_size', type=int, default=100000, metavar='DS',\n","                    help='size of data set (default: 100000)')\n","parser.add_argument('--high_level_feats', type=bool, default=None, metavar='HLF',\n","                    help='toggles high level features (default: None)')\n","parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n","                    help='input batch size for training (default: 64)')\n","parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                    help='input batch size for testing (default: 1000)')\n","parser.add_argument('--epochs', type=int, default=10, metavar='N',\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n","                    help='learning rate (default: 0.1)')\n","parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n","                    help='SGD momentum (default: 0.5)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='disables CUDA training')\n","parser.add_argument('--seed', type=int, default=2, metavar='S',\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='how many batches to wait before logging training status')\n","parser.add_argument('--layers', type=int, default=1, metavar='N',\n","                    help='number of layers of NN')\n","parser.add_argument('--single_layer', type=bool, default=False, metavar='HLF',\n","                    help='use 1000 or 100')\n","parser.add_argument('--use_cuda', type=bool, default=False, metavar='HLF',\n","                    help='use cuda or cpu')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"939m8ZrSMa0p"},"source":["### 第一小问\n"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3697246,"status":"ok","timestamp":1672797392451,"user":{"displayName":"Bruce Wong","userId":"14772981164940940158"},"user_tz":-480},"id":"g8T8w4PwMXJy","outputId":"8094becd-b2ae-41e7-87b5-814effda4e9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on 800 examples\n","Using both high and low level features\n","Testing on 200 examples\n","Using both high and low level features\n","\n"," training DNN with 1000 data points. \n","\n","Train Epoch: 1 [0/800 (0%)]\tLoss: 0.762958\n","Train Epoch: 1 [100/800 (12%)]\tLoss: 17.841774\n","Train Epoch: 1 [200/800 (25%)]\tLoss: 20.325108\n","Train Epoch: 1 [300/800 (38%)]\tLoss: 5.945452\n","Train Epoch: 1 [400/800 (50%)]\tLoss: 24.140615\n","Train Epoch: 1 [500/800 (62%)]\tLoss: 12.679227\n","Train Epoch: 1 [600/800 (75%)]\tLoss: 62.166859\n","Train Epoch: 1 [700/800 (88%)]\tLoss: 6.689935\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test set: Average loss: 10.9656, Accuracy: 127/200 (63.500%)\n","\n","Train Epoch: 2 [0/800 (0%)]\tLoss: 11.266635\n","Train Epoch: 2 [100/800 (12%)]\tLoss: 13.048929\n","Train Epoch: 2 [200/800 (25%)]\tLoss: 0.632492\n","Train Epoch: 2 [300/800 (38%)]\tLoss: 13.456708\n","Train Epoch: 2 [400/800 (50%)]\tLoss: 4.275938\n","Train Epoch: 2 [500/800 (62%)]\tLoss: 9.510625\n","Train Epoch: 2 [600/800 (75%)]\tLoss: 18.014854\n","Train Epoch: 2 [700/800 (88%)]\tLoss: 5.799457\n","\n","Test set: Average loss: 4.9244, Accuracy: 135/200 (67.500%)\n","\n","Train Epoch: 3 [0/800 (0%)]\tLoss: 8.331659\n","Train Epoch: 3 [100/800 (12%)]\tLoss: 3.856112\n","Train Epoch: 3 [200/800 (25%)]\tLoss: 3.943788\n","Train Epoch: 3 [300/800 (38%)]\tLoss: 4.522349\n","Train Epoch: 3 [400/800 (50%)]\tLoss: 0.018233\n","Train Epoch: 3 [500/800 (62%)]\tLoss: 8.750836\n","Train Epoch: 3 [600/800 (75%)]\tLoss: 11.190210\n","Train Epoch: 3 [700/800 (88%)]\tLoss: 3.317886\n","\n","Test set: Average loss: 1.2537, Accuracy: 149/200 (74.500%)\n","\n","Train Epoch: 4 [0/800 (0%)]\tLoss: 1.036048\n","Train Epoch: 4 [100/800 (12%)]\tLoss: 2.859593\n","Train Epoch: 4 [200/800 (25%)]\tLoss: 1.796613\n","Train Epoch: 4 [300/800 (38%)]\tLoss: 4.063908\n","Train Epoch: 4 [400/800 (50%)]\tLoss: 3.282791\n","Train Epoch: 4 [500/800 (62%)]\tLoss: 0.984802\n","Train Epoch: 4 [600/800 (75%)]\tLoss: 0.039599\n","Train Epoch: 4 [700/800 (88%)]\tLoss: 1.949210\n","\n","Test set: Average loss: 1.5162, Accuracy: 140/200 (70.000%)\n","\n","Train Epoch: 5 [0/800 (0%)]\tLoss: 2.633349\n","Train Epoch: 5 [100/800 (12%)]\tLoss: 3.961985\n","Train Epoch: 5 [200/800 (25%)]\tLoss: 1.452422\n","Train Epoch: 5 [300/800 (38%)]\tLoss: 2.152534\n","Train Epoch: 5 [400/800 (50%)]\tLoss: 0.807105\n","Train Epoch: 5 [500/800 (62%)]\tLoss: 1.951414\n","Train Epoch: 5 [600/800 (75%)]\tLoss: 1.802012\n","Train Epoch: 5 [700/800 (88%)]\tLoss: 1.085470\n","\n","Test set: Average loss: 1.4330, Accuracy: 137/200 (68.500%)\n","\n","Train Epoch: 6 [0/800 (0%)]\tLoss: 0.825237\n","Train Epoch: 6 [100/800 (12%)]\tLoss: 1.843951\n","Train Epoch: 6 [200/800 (25%)]\tLoss: 0.853478\n","Train Epoch: 6 [300/800 (38%)]\tLoss: 0.126782\n","Train Epoch: 6 [400/800 (50%)]\tLoss: 0.509883\n","Train Epoch: 6 [500/800 (62%)]\tLoss: 0.660149\n","Train Epoch: 6 [600/800 (75%)]\tLoss: 0.494037\n","Train Epoch: 6 [700/800 (88%)]\tLoss: 0.701720\n","\n","Test set: Average loss: 1.2939, Accuracy: 136/200 (68.000%)\n","\n","Train Epoch: 7 [0/800 (0%)]\tLoss: 0.715806\n","Train Epoch: 7 [100/800 (12%)]\tLoss: 0.288028\n","Train Epoch: 7 [200/800 (25%)]\tLoss: 1.061871\n","Train Epoch: 7 [300/800 (38%)]\tLoss: 0.494365\n","Train Epoch: 7 [400/800 (50%)]\tLoss: 0.668167\n","Train Epoch: 7 [500/800 (62%)]\tLoss: 0.276892\n","Train Epoch: 7 [600/800 (75%)]\tLoss: 0.357889\n","Train Epoch: 7 [700/800 (88%)]\tLoss: 0.555411\n","\n","Test set: Average loss: 2.6873, Accuracy: 142/200 (71.000%)\n","\n","Train Epoch: 8 [0/800 (0%)]\tLoss: 0.166899\n","Train Epoch: 8 [100/800 (12%)]\tLoss: 0.553641\n","Train Epoch: 8 [200/800 (25%)]\tLoss: 0.583670\n","Train Epoch: 8 [300/800 (38%)]\tLoss: 0.657658\n","Train Epoch: 8 [400/800 (50%)]\tLoss: 0.847139\n","Train Epoch: 8 [500/800 (62%)]\tLoss: 0.667995\n","Train Epoch: 8 [600/800 (75%)]\tLoss: 0.346018\n","Train Epoch: 8 [700/800 (88%)]\tLoss: 0.691607\n","\n","Test set: Average loss: 2.8354, Accuracy: 133/200 (66.500%)\n","\n","Train Epoch: 9 [0/800 (0%)]\tLoss: 0.712891\n","Train Epoch: 9 [100/800 (12%)]\tLoss: 0.368486\n","Train Epoch: 9 [200/800 (25%)]\tLoss: 0.826891\n","Train Epoch: 9 [300/800 (38%)]\tLoss: 0.580578\n","Train Epoch: 9 [400/800 (50%)]\tLoss: 0.107019\n","Train Epoch: 9 [500/800 (62%)]\tLoss: 0.568309\n","Train Epoch: 9 [600/800 (75%)]\tLoss: 0.280227\n","Train Epoch: 9 [700/800 (88%)]\tLoss: 0.249855\n","\n","Test set: Average loss: 0.6107, Accuracy: 146/200 (73.000%)\n","\n","Train Epoch: 10 [0/800 (0%)]\tLoss: 0.625910\n","Train Epoch: 10 [100/800 (12%)]\tLoss: 0.392225\n","Train Epoch: 10 [200/800 (25%)]\tLoss: 0.796552\n","Train Epoch: 10 [300/800 (38%)]\tLoss: 1.224319\n","Train Epoch: 10 [400/800 (50%)]\tLoss: 0.638930\n","Train Epoch: 10 [500/800 (62%)]\tLoss: 0.417097\n","Train Epoch: 10 [600/800 (75%)]\tLoss: 0.556018\n","Train Epoch: 10 [700/800 (88%)]\tLoss: 0.642204\n","\n","Test set: Average loss: 0.8394, Accuracy: 114/200 (57.000%)\n","\n","Training on 900600 examples\n","Using both high and low level features\n","Testing on 225150 examples\n","Using both high and low level features\n","\n"," training DNN with 1125750 data points. \n","\n","Train Epoch: 1 [0/900600 (0%)]\tLoss: 0.768113\n","Train Epoch: 1 [112570/900600 (12%)]\tLoss: 0.514151\n","Train Epoch: 1 [225140/900600 (25%)]\tLoss: 0.482047\n","Train Epoch: 1 [337710/900600 (37%)]\tLoss: 0.470995\n","Train Epoch: 1 [450280/900600 (49%)]\tLoss: 0.465685\n","Train Epoch: 1 [562850/900600 (62%)]\tLoss: 0.461709\n","Train Epoch: 1 [675420/900600 (74%)]\tLoss: 0.445349\n","Train Epoch: 1 [787990/900600 (86%)]\tLoss: 0.453978\n","Train Epoch: 1 [3200/900600 (99%)]\tLoss: 0.291511\n","\n","Test set: Average loss: 0.5696, Accuracy: 166156/225150 (73.798%)\n","\n","Train Epoch: 2 [0/900600 (0%)]\tLoss: 0.588805\n","Train Epoch: 2 [112570/900600 (12%)]\tLoss: 0.499306\n","Train Epoch: 2 [225140/900600 (25%)]\tLoss: 0.458759\n","Train Epoch: 2 [337710/900600 (37%)]\tLoss: 0.452636\n","Train Epoch: 2 [450280/900600 (49%)]\tLoss: 0.454236\n","Train Epoch: 2 [562850/900600 (62%)]\tLoss: 0.465387\n","Train Epoch: 2 [675420/900600 (74%)]\tLoss: 0.450414\n","Train Epoch: 2 [787990/900600 (86%)]\tLoss: 0.448467\n","Train Epoch: 2 [3200/900600 (99%)]\tLoss: 0.481876\n","\n","Test set: Average loss: 0.7130, Accuracy: 154202/225150 (68.489%)\n","\n","Train Epoch: 3 [0/900600 (0%)]\tLoss: 0.743523\n","Train Epoch: 3 [112570/900600 (12%)]\tLoss: 0.545478\n","Train Epoch: 3 [225140/900600 (25%)]\tLoss: 0.464823\n","Train Epoch: 3 [337710/900600 (37%)]\tLoss: 0.456417\n","Train Epoch: 3 [450280/900600 (49%)]\tLoss: 0.459876\n","Train Epoch: 3 [562850/900600 (62%)]\tLoss: 0.453470\n","Train Epoch: 3 [675420/900600 (74%)]\tLoss: 0.450059\n","Train Epoch: 3 [787990/900600 (86%)]\tLoss: 0.457726\n","Train Epoch: 3 [3200/900600 (99%)]\tLoss: 0.552726\n","\n","Test set: Average loss: 0.9072, Accuracy: 134862/225150 (59.899%)\n","\n","Train Epoch: 4 [0/900600 (0%)]\tLoss: 0.961896\n","Train Epoch: 4 [112570/900600 (12%)]\tLoss: 0.598236\n","Train Epoch: 4 [225140/900600 (25%)]\tLoss: 0.468875\n","Train Epoch: 4 [337710/900600 (37%)]\tLoss: 0.446972\n","Train Epoch: 4 [450280/900600 (49%)]\tLoss: 0.451523\n","Train Epoch: 4 [562850/900600 (62%)]\tLoss: 0.456685\n","Train Epoch: 4 [675420/900600 (74%)]\tLoss: 0.451945\n","Train Epoch: 4 [787990/900600 (86%)]\tLoss: 0.453895\n","Train Epoch: 4 [3200/900600 (99%)]\tLoss: 0.295617\n","\n","Test set: Average loss: 0.6192, Accuracy: 168680/225150 (74.919%)\n","\n","Train Epoch: 5 [0/900600 (0%)]\tLoss: 0.609844\n","Train Epoch: 5 [112570/900600 (12%)]\tLoss: 0.497790\n","Train Epoch: 5 [225140/900600 (25%)]\tLoss: 0.456918\n","Train Epoch: 5 [337710/900600 (37%)]\tLoss: 0.445109\n","Train Epoch: 5 [450280/900600 (49%)]\tLoss: 0.474704\n","Train Epoch: 5 [562850/900600 (62%)]\tLoss: 0.448909\n","Train Epoch: 5 [675420/900600 (74%)]\tLoss: 0.451382\n","Train Epoch: 5 [787990/900600 (86%)]\tLoss: 0.445611\n","Train Epoch: 5 [3200/900600 (99%)]\tLoss: 0.379511\n","\n","Test set: Average loss: 0.8792, Accuracy: 155936/225150 (69.259%)\n","\n","Train Epoch: 6 [0/900600 (0%)]\tLoss: 0.898540\n","Train Epoch: 6 [112570/900600 (12%)]\tLoss: 0.551391\n","Train Epoch: 6 [225140/900600 (25%)]\tLoss: 0.457368\n","Train Epoch: 6 [337710/900600 (37%)]\tLoss: 0.451389\n","Train Epoch: 6 [450280/900600 (49%)]\tLoss: 0.452533\n","Train Epoch: 6 [562850/900600 (62%)]\tLoss: 0.446201\n","Train Epoch: 6 [675420/900600 (74%)]\tLoss: 0.456954\n","Train Epoch: 6 [787990/900600 (86%)]\tLoss: 0.456205\n","Train Epoch: 6 [3200/900600 (99%)]\tLoss: 0.422975\n","\n","Test set: Average loss: 0.5885, Accuracy: 159450/225150 (70.819%)\n","\n","Train Epoch: 7 [0/900600 (0%)]\tLoss: 0.601974\n","Train Epoch: 7 [112570/900600 (12%)]\tLoss: 0.508399\n","Train Epoch: 7 [225140/900600 (25%)]\tLoss: 0.458046\n","Train Epoch: 7 [337710/900600 (37%)]\tLoss: 0.458107\n","Train Epoch: 7 [450280/900600 (49%)]\tLoss: 0.456125\n","Train Epoch: 7 [562850/900600 (62%)]\tLoss: 0.453619\n","Train Epoch: 7 [675420/900600 (74%)]\tLoss: 0.452989\n","Train Epoch: 7 [787990/900600 (86%)]\tLoss: 0.447516\n","Train Epoch: 7 [3200/900600 (99%)]\tLoss: 0.432190\n","\n","Test set: Average loss: 0.6281, Accuracy: 163090/225150 (72.436%)\n","\n","Train Epoch: 8 [0/900600 (0%)]\tLoss: 0.645703\n","Train Epoch: 8 [112570/900600 (12%)]\tLoss: 0.502829\n","Train Epoch: 8 [225140/900600 (25%)]\tLoss: 0.459406\n","Train Epoch: 8 [337710/900600 (37%)]\tLoss: 0.453499\n","Train Epoch: 8 [450280/900600 (49%)]\tLoss: 0.455453\n","Train Epoch: 8 [562850/900600 (62%)]\tLoss: 0.443566\n","Train Epoch: 8 [675420/900600 (74%)]\tLoss: 0.446108\n","Train Epoch: 8 [787990/900600 (86%)]\tLoss: 0.448708\n","Train Epoch: 8 [3200/900600 (99%)]\tLoss: 0.693772\n","\n","Test set: Average loss: 0.6341, Accuracy: 156715/225150 (69.605%)\n","\n","Train Epoch: 9 [0/900600 (0%)]\tLoss: 0.655009\n","Train Epoch: 9 [112570/900600 (12%)]\tLoss: 0.547418\n","Train Epoch: 9 [225140/900600 (25%)]\tLoss: 0.471720\n","Train Epoch: 9 [337710/900600 (37%)]\tLoss: 0.453117\n","Train Epoch: 9 [450280/900600 (49%)]\tLoss: 0.452269\n","Train Epoch: 9 [562850/900600 (62%)]\tLoss: 0.442081\n","Train Epoch: 9 [675420/900600 (74%)]\tLoss: 0.441032\n","Train Epoch: 9 [787990/900600 (86%)]\tLoss: 0.452489\n","Train Epoch: 9 [3200/900600 (99%)]\tLoss: 0.366993\n","\n","Test set: Average loss: 0.4749, Accuracy: 174508/225150 (77.507%)\n","\n","Train Epoch: 10 [0/900600 (0%)]\tLoss: 0.495225\n","Train Epoch: 10 [112570/900600 (12%)]\tLoss: 0.488751\n","Train Epoch: 10 [225140/900600 (25%)]\tLoss: 0.445914\n","Train Epoch: 10 [337710/900600 (37%)]\tLoss: 0.449033\n","Train Epoch: 10 [450280/900600 (49%)]\tLoss: 0.441134\n","Train Epoch: 10 [562850/900600 (62%)]\tLoss: 0.442973\n","Train Epoch: 10 [675420/900600 (74%)]\tLoss: 0.456564\n","Train Epoch: 10 [787990/900600 (86%)]\tLoss: 0.444300\n","Train Epoch: 10 [3200/900600 (99%)]\tLoss: 0.359935\n","\n","Test set: Average loss: 0.6222, Accuracy: 160253/225150 (71.176%)\n","\n","Training on 1800400 examples\n","Using both high and low level features\n","Testing on 450100 examples\n","Using both high and low level features\n","\n"," training DNN with 2250500 data points. \n","\n","Train Epoch: 1 [0/1800400 (0%)]\tLoss: 0.824419\n","Train Epoch: 1 [225050/1800400 (12%)]\tLoss: 0.508846\n","Train Epoch: 1 [450100/1800400 (25%)]\tLoss: 0.474251\n","Train Epoch: 1 [675150/1800400 (38%)]\tLoss: 0.461595\n","Train Epoch: 1 [900200/1800400 (50%)]\tLoss: 0.465225\n","Train Epoch: 1 [1125250/1800400 (62%)]\tLoss: 0.455869\n","Train Epoch: 1 [1350300/1800400 (75%)]\tLoss: 0.458099\n","Train Epoch: 1 [1575350/1800400 (88%)]\tLoss: 0.462318\n","\n","Test set: Average loss: 0.4411, Accuracy: 358882/450100 (79.734%)\n","\n","Train Epoch: 2 [0/1800400 (0%)]\tLoss: 0.454304\n","Train Epoch: 2 [225050/1800400 (12%)]\tLoss: 0.459859\n","Train Epoch: 2 [450100/1800400 (25%)]\tLoss: 0.458848\n","Train Epoch: 2 [675150/1800400 (38%)]\tLoss: 0.452942\n","Train Epoch: 2 [900200/1800400 (50%)]\tLoss: 0.448770\n","Train Epoch: 2 [1125250/1800400 (62%)]\tLoss: 0.454154\n","Train Epoch: 2 [1350300/1800400 (75%)]\tLoss: 0.452297\n","Train Epoch: 2 [1575350/1800400 (88%)]\tLoss: 0.452667\n","\n","Test set: Average loss: 0.4391, Accuracy: 359077/450100 (79.777%)\n","\n","Train Epoch: 3 [0/1800400 (0%)]\tLoss: 0.451814\n","Train Epoch: 3 [225050/1800400 (12%)]\tLoss: 0.443202\n","Train Epoch: 3 [450100/1800400 (25%)]\tLoss: 0.452955\n","Train Epoch: 3 [675150/1800400 (38%)]\tLoss: 0.450966\n","Train Epoch: 3 [900200/1800400 (50%)]\tLoss: 0.447841\n","Train Epoch: 3 [1125250/1800400 (62%)]\tLoss: 0.456313\n","Train Epoch: 3 [1350300/1800400 (75%)]\tLoss: 0.450169\n","Train Epoch: 3 [1575350/1800400 (88%)]\tLoss: 0.450155\n","\n","Test set: Average loss: 0.4394, Accuracy: 359000/450100 (79.760%)\n","\n","Train Epoch: 4 [0/1800400 (0%)]\tLoss: 0.454306\n","Train Epoch: 4 [225050/1800400 (12%)]\tLoss: 0.446958\n","Train Epoch: 4 [450100/1800400 (25%)]\tLoss: 0.452482\n","Train Epoch: 4 [675150/1800400 (38%)]\tLoss: 0.445608\n","Train Epoch: 4 [900200/1800400 (50%)]\tLoss: 0.449899\n","Train Epoch: 4 [1125250/1800400 (62%)]\tLoss: 0.454361\n","Train Epoch: 4 [1350300/1800400 (75%)]\tLoss: 0.451672\n","Train Epoch: 4 [1575350/1800400 (88%)]\tLoss: 0.446948\n","\n","Test set: Average loss: 0.4404, Accuracy: 358572/450100 (79.665%)\n","\n","Train Epoch: 5 [0/1800400 (0%)]\tLoss: 0.451638\n","Train Epoch: 5 [225050/1800400 (12%)]\tLoss: 0.452439\n","Train Epoch: 5 [450100/1800400 (25%)]\tLoss: 0.449250\n","Train Epoch: 5 [675150/1800400 (38%)]\tLoss: 0.440401\n","Train Epoch: 5 [900200/1800400 (50%)]\tLoss: 0.449884\n","Train Epoch: 5 [1125250/1800400 (62%)]\tLoss: 0.450282\n","Train Epoch: 5 [1350300/1800400 (75%)]\tLoss: 0.439156\n","Train Epoch: 5 [1575350/1800400 (88%)]\tLoss: 0.449215\n","\n","Test set: Average loss: 0.4376, Accuracy: 359263/450100 (79.818%)\n","\n","Train Epoch: 6 [0/1800400 (0%)]\tLoss: 0.441175\n","Train Epoch: 6 [225050/1800400 (12%)]\tLoss: 0.447354\n","Train Epoch: 6 [450100/1800400 (25%)]\tLoss: 0.450881\n","Train Epoch: 6 [675150/1800400 (38%)]\tLoss: 0.445683\n","Train Epoch: 6 [900200/1800400 (50%)]\tLoss: 0.449553\n","Train Epoch: 6 [1125250/1800400 (62%)]\tLoss: 0.443617\n","Train Epoch: 6 [1350300/1800400 (75%)]\tLoss: 0.446838\n","Train Epoch: 6 [1575350/1800400 (88%)]\tLoss: 0.444682\n","\n","Test set: Average loss: 0.4363, Accuracy: 359482/450100 (79.867%)\n","\n","Train Epoch: 7 [0/1800400 (0%)]\tLoss: 0.444078\n","Train Epoch: 7 [225050/1800400 (12%)]\tLoss: 0.438559\n","Train Epoch: 7 [450100/1800400 (25%)]\tLoss: 0.447571\n","Train Epoch: 7 [675150/1800400 (38%)]\tLoss: 0.445345\n","Train Epoch: 7 [900200/1800400 (50%)]\tLoss: 0.442463\n","Train Epoch: 7 [1125250/1800400 (62%)]\tLoss: 0.450230\n","Train Epoch: 7 [1350300/1800400 (75%)]\tLoss: 0.439134\n","Train Epoch: 7 [1575350/1800400 (88%)]\tLoss: 0.444267\n","\n","Test set: Average loss: 0.4349, Accuracy: 359871/450100 (79.954%)\n","\n","Train Epoch: 8 [0/1800400 (0%)]\tLoss: 0.445656\n","Train Epoch: 8 [225050/1800400 (12%)]\tLoss: 0.445788\n","Train Epoch: 8 [450100/1800400 (25%)]\tLoss: 0.446769\n","Train Epoch: 8 [675150/1800400 (38%)]\tLoss: 0.449249\n","Train Epoch: 8 [900200/1800400 (50%)]\tLoss: 0.444118\n","Train Epoch: 8 [1125250/1800400 (62%)]\tLoss: 0.440879\n","Train Epoch: 8 [1350300/1800400 (75%)]\tLoss: 0.440247\n","Train Epoch: 8 [1575350/1800400 (88%)]\tLoss: 0.442026\n","\n","Test set: Average loss: 0.4351, Accuracy: 359991/450100 (79.980%)\n","\n","Train Epoch: 9 [0/1800400 (0%)]\tLoss: 0.441175\n","Train Epoch: 9 [225050/1800400 (12%)]\tLoss: 0.450248\n","Train Epoch: 9 [450100/1800400 (25%)]\tLoss: 0.442389\n","Train Epoch: 9 [675150/1800400 (38%)]\tLoss: 0.442039\n","Train Epoch: 9 [900200/1800400 (50%)]\tLoss: 0.448471\n","Train Epoch: 9 [1125250/1800400 (62%)]\tLoss: 0.453834\n","Train Epoch: 9 [1350300/1800400 (75%)]\tLoss: 0.450805\n","Train Epoch: 9 [1575350/1800400 (88%)]\tLoss: 0.441114\n","\n","Test set: Average loss: 0.4345, Accuracy: 359777/450100 (79.933%)\n","\n","Train Epoch: 10 [0/1800400 (0%)]\tLoss: 0.445492\n","Train Epoch: 10 [225050/1800400 (12%)]\tLoss: 0.436020\n","Train Epoch: 10 [450100/1800400 (25%)]\tLoss: 0.443474\n","Train Epoch: 10 [675150/1800400 (38%)]\tLoss: 0.444956\n","Train Epoch: 10 [900200/1800400 (50%)]\tLoss: 0.446981\n","Train Epoch: 10 [1125250/1800400 (62%)]\tLoss: 0.447040\n","Train Epoch: 10 [1350300/1800400 (75%)]\tLoss: 0.443282\n","Train Epoch: 10 [1575350/1800400 (88%)]\tLoss: 0.445057\n","\n","Test set: Average loss: 0.4371, Accuracy: 359211/450100 (79.807%)\n","\n","Training on 2700200 examples\n","Using both high and low level features\n","Testing on 675050 examples\n","Using both high and low level features\n","\n"," training DNN with 3375250 data points. \n","\n","Train Epoch: 1 [0/2700200 (0%)]\tLoss: 0.824213\n","Train Epoch: 1 [337520/2700200 (12%)]\tLoss: 0.532835\n","Train Epoch: 1 [675040/2700200 (25%)]\tLoss: 0.464598\n","Train Epoch: 1 [1012560/2700200 (37%)]\tLoss: 0.462035\n","Train Epoch: 1 [1350080/2700200 (49%)]\tLoss: 0.459314\n","Train Epoch: 1 [1687600/2700200 (62%)]\tLoss: 0.460194\n","Train Epoch: 1 [2025120/2700200 (74%)]\tLoss: 0.457257\n","Train Epoch: 1 [2362640/2700200 (86%)]\tLoss: 0.453916\n","Train Epoch: 1 [3200/2700200 (99%)]\tLoss: 0.536220\n","\n","Test set: Average loss: 0.7466, Accuracy: 455575/675050 (67.488%)\n","\n","Train Epoch: 2 [0/2700200 (0%)]\tLoss: 0.771433\n","Train Epoch: 2 [337520/2700200 (12%)]\tLoss: 0.571443\n","Train Epoch: 2 [675040/2700200 (25%)]\tLoss: 0.467610\n","Train Epoch: 2 [1012560/2700200 (37%)]\tLoss: 0.453119\n","Train Epoch: 2 [1350080/2700200 (49%)]\tLoss: 0.460824\n","Train Epoch: 2 [1687600/2700200 (62%)]\tLoss: 0.453713\n","Train Epoch: 2 [2025120/2700200 (74%)]\tLoss: 0.450079\n","Train Epoch: 2 [2362640/2700200 (86%)]\tLoss: 0.452406\n","Train Epoch: 2 [3200/2700200 (99%)]\tLoss: 0.355241\n","\n","Test set: Average loss: 0.5610, Accuracy: 503473/675050 (74.583%)\n","\n","Train Epoch: 3 [0/2700200 (0%)]\tLoss: 0.568684\n","Train Epoch: 3 [337520/2700200 (12%)]\tLoss: 0.495103\n","Train Epoch: 3 [675040/2700200 (25%)]\tLoss: 0.454276\n","Train Epoch: 3 [1012560/2700200 (37%)]\tLoss: 0.450566\n","Train Epoch: 3 [1350080/2700200 (49%)]\tLoss: 0.449810\n","Train Epoch: 3 [1687600/2700200 (62%)]\tLoss: 0.447522\n","Train Epoch: 3 [2025120/2700200 (74%)]\tLoss: 0.449295\n","Train Epoch: 3 [2362640/2700200 (86%)]\tLoss: 0.447938\n","Train Epoch: 3 [3200/2700200 (99%)]\tLoss: 0.583135\n","\n","Test set: Average loss: 0.8978, Accuracy: 422716/675050 (62.620%)\n","\n","Train Epoch: 4 [0/2700200 (0%)]\tLoss: 0.922227\n","Train Epoch: 4 [337520/2700200 (12%)]\tLoss: 0.560176\n","Train Epoch: 4 [675040/2700200 (25%)]\tLoss: 0.465601\n","Train Epoch: 4 [1012560/2700200 (37%)]\tLoss: 0.447934\n","Train Epoch: 4 [1350080/2700200 (49%)]\tLoss: 0.447790\n","Train Epoch: 4 [1687600/2700200 (62%)]\tLoss: 0.450864\n","Train Epoch: 4 [2025120/2700200 (74%)]\tLoss: 0.444283\n","Train Epoch: 4 [2362640/2700200 (86%)]\tLoss: 0.449761\n","Train Epoch: 4 [3200/2700200 (99%)]\tLoss: 0.504189\n","\n","Test set: Average loss: 0.7594, Accuracy: 469880/675050 (69.607%)\n","\n","Train Epoch: 5 [0/2700200 (0%)]\tLoss: 0.781856\n","Train Epoch: 5 [337520/2700200 (12%)]\tLoss: 0.546863\n","Train Epoch: 5 [675040/2700200 (25%)]\tLoss: 0.460222\n","Train Epoch: 5 [1012560/2700200 (37%)]\tLoss: 0.452187\n","Train Epoch: 5 [1350080/2700200 (49%)]\tLoss: 0.446789\n","Train Epoch: 5 [1687600/2700200 (62%)]\tLoss: 0.445037\n","Train Epoch: 5 [2025120/2700200 (74%)]\tLoss: 0.444954\n","Train Epoch: 5 [2362640/2700200 (86%)]\tLoss: 0.449302\n","Train Epoch: 5 [3200/2700200 (99%)]\tLoss: 0.483522\n","\n","Test set: Average loss: 0.6279, Accuracy: 494051/675050 (73.187%)\n","\n","Train Epoch: 6 [0/2700200 (0%)]\tLoss: 0.640997\n","Train Epoch: 6 [337520/2700200 (12%)]\tLoss: 0.523210\n","Train Epoch: 6 [675040/2700200 (25%)]\tLoss: 0.451122\n","Train Epoch: 6 [1012560/2700200 (37%)]\tLoss: 0.441913\n","Train Epoch: 6 [1350080/2700200 (49%)]\tLoss: 0.444309\n","Train Epoch: 6 [1687600/2700200 (62%)]\tLoss: 0.440463\n","Train Epoch: 6 [2025120/2700200 (74%)]\tLoss: 0.447812\n","Train Epoch: 6 [2362640/2700200 (86%)]\tLoss: 0.445961\n","Train Epoch: 6 [3200/2700200 (99%)]\tLoss: 0.463657\n","\n","Test set: Average loss: 0.8392, Accuracy: 449767/675050 (66.627%)\n","\n","Train Epoch: 7 [0/2700200 (0%)]\tLoss: 0.847379\n","Train Epoch: 7 [337520/2700200 (12%)]\tLoss: 0.553620\n","Train Epoch: 7 [675040/2700200 (25%)]\tLoss: 0.454924\n","Train Epoch: 7 [1012560/2700200 (37%)]\tLoss: 0.449058\n","Train Epoch: 7 [1350080/2700200 (49%)]\tLoss: 0.446997\n","Train Epoch: 7 [1687600/2700200 (62%)]\tLoss: 0.444928\n","Train Epoch: 7 [2025120/2700200 (74%)]\tLoss: 0.443553\n","Train Epoch: 7 [2362640/2700200 (86%)]\tLoss: 0.445754\n","Train Epoch: 7 [3200/2700200 (99%)]\tLoss: 0.477779\n","\n","Test set: Average loss: 0.6474, Accuracy: 482218/675050 (71.434%)\n","\n","Train Epoch: 8 [0/2700200 (0%)]\tLoss: 0.659918\n","Train Epoch: 8 [337520/2700200 (12%)]\tLoss: 0.513789\n","Train Epoch: 8 [675040/2700200 (25%)]\tLoss: 0.454819\n","Train Epoch: 8 [1012560/2700200 (37%)]\tLoss: 0.446710\n","Train Epoch: 8 [1350080/2700200 (49%)]\tLoss: 0.443757\n","Train Epoch: 8 [1687600/2700200 (62%)]\tLoss: 0.449431\n","Train Epoch: 8 [2025120/2700200 (74%)]\tLoss: 0.442852\n","Train Epoch: 8 [2362640/2700200 (86%)]\tLoss: 0.439396\n","Train Epoch: 8 [3200/2700200 (99%)]\tLoss: 0.447121\n","\n","Test set: Average loss: 0.7046, Accuracy: 478737/675050 (70.919%)\n","\n","Train Epoch: 9 [0/2700200 (0%)]\tLoss: 0.719302\n","Train Epoch: 9 [337520/2700200 (12%)]\tLoss: 0.526973\n","Train Epoch: 9 [675040/2700200 (25%)]\tLoss: 0.454164\n","Train Epoch: 9 [1012560/2700200 (37%)]\tLoss: 0.445868\n","Train Epoch: 9 [1350080/2700200 (49%)]\tLoss: 0.444034\n","Train Epoch: 9 [1687600/2700200 (62%)]\tLoss: 0.433979\n","Train Epoch: 9 [2025120/2700200 (74%)]\tLoss: 0.446291\n","Train Epoch: 9 [2362640/2700200 (86%)]\tLoss: 0.444949\n","Train Epoch: 9 [3200/2700200 (99%)]\tLoss: 0.517444\n","\n","Test set: Average loss: 0.6362, Accuracy: 453262/675050 (67.145%)\n","\n","Train Epoch: 10 [0/2700200 (0%)]\tLoss: 0.650455\n","Train Epoch: 10 [337520/2700200 (12%)]\tLoss: 0.507771\n","Train Epoch: 10 [675040/2700200 (25%)]\tLoss: 0.450414\n","Train Epoch: 10 [1012560/2700200 (37%)]\tLoss: 0.440278\n","Train Epoch: 10 [1350080/2700200 (49%)]\tLoss: 0.446206\n","Train Epoch: 10 [1687600/2700200 (62%)]\tLoss: 0.443112\n","Train Epoch: 10 [2025120/2700200 (74%)]\tLoss: 0.442655\n","Train Epoch: 10 [2362640/2700200 (86%)]\tLoss: 0.439755\n","Train Epoch: 10 [3200/2700200 (99%)]\tLoss: 0.348406\n","\n","Test set: Average loss: 0.5042, Accuracy: 516074/675050 (76.450%)\n","\n","Training on 3600000 examples\n","Using both high and low level features\n","Testing on 900000 examples\n","Using both high and low level features\n","\n"," training DNN with 4500000 data points. \n","\n","Train Epoch: 1 [0/3600000 (0%)]\tLoss: 0.704530\n","Train Epoch: 1 [450000/3600000 (12%)]\tLoss: 0.494367\n","Train Epoch: 1 [900000/3600000 (25%)]\tLoss: 0.459977\n","Train Epoch: 1 [1350000/3600000 (38%)]\tLoss: 0.456950\n","Train Epoch: 1 [1800000/3600000 (50%)]\tLoss: 0.455066\n","Train Epoch: 1 [2250000/3600000 (62%)]\tLoss: 0.459260\n","Train Epoch: 1 [2700000/3600000 (75%)]\tLoss: 0.454691\n","Train Epoch: 1 [3150000/3600000 (88%)]\tLoss: 0.452046\n","\n","Test set: Average loss: 0.4403, Accuracy: 717798/900000 (79.755%)\n","\n","Train Epoch: 2 [0/3600000 (0%)]\tLoss: 0.451741\n","Train Epoch: 2 [450000/3600000 (12%)]\tLoss: 0.453684\n","Train Epoch: 2 [900000/3600000 (25%)]\tLoss: 0.450476\n","Train Epoch: 2 [1350000/3600000 (38%)]\tLoss: 0.454830\n","Train Epoch: 2 [1800000/3600000 (50%)]\tLoss: 0.449504\n","Train Epoch: 2 [2250000/3600000 (62%)]\tLoss: 0.452516\n","Train Epoch: 2 [2700000/3600000 (75%)]\tLoss: 0.456568\n","Train Epoch: 2 [3150000/3600000 (88%)]\tLoss: 0.449263\n","\n","Test set: Average loss: 0.4388, Accuracy: 718144/900000 (79.794%)\n","\n","Train Epoch: 3 [0/3600000 (0%)]\tLoss: 0.447779\n","Train Epoch: 3 [450000/3600000 (12%)]\tLoss: 0.454044\n","Train Epoch: 3 [900000/3600000 (25%)]\tLoss: 0.451708\n","Train Epoch: 3 [1350000/3600000 (38%)]\tLoss: 0.450436\n","Train Epoch: 3 [1800000/3600000 (50%)]\tLoss: 0.445991\n","Train Epoch: 3 [2250000/3600000 (62%)]\tLoss: 0.449873\n","Train Epoch: 3 [2700000/3600000 (75%)]\tLoss: 0.449670\n","Train Epoch: 3 [3150000/3600000 (88%)]\tLoss: 0.448790\n","\n","Test set: Average loss: 0.4365, Accuracy: 718807/900000 (79.867%)\n","\n","Train Epoch: 4 [0/3600000 (0%)]\tLoss: 0.453054\n","Train Epoch: 4 [450000/3600000 (12%)]\tLoss: 0.445749\n","Train Epoch: 4 [900000/3600000 (25%)]\tLoss: 0.445596\n","Train Epoch: 4 [1350000/3600000 (38%)]\tLoss: 0.442405\n","Train Epoch: 4 [1800000/3600000 (50%)]\tLoss: 0.444470\n","Train Epoch: 4 [2250000/3600000 (62%)]\tLoss: 0.449854\n","Train Epoch: 4 [2700000/3600000 (75%)]\tLoss: 0.447824\n","Train Epoch: 4 [3150000/3600000 (88%)]\tLoss: 0.444696\n","\n","Test set: Average loss: 0.4361, Accuracy: 719048/900000 (79.894%)\n","\n","Train Epoch: 5 [0/3600000 (0%)]\tLoss: 0.444727\n","Train Epoch: 5 [450000/3600000 (12%)]\tLoss: 0.446019\n","Train Epoch: 5 [900000/3600000 (25%)]\tLoss: 0.442247\n","Train Epoch: 5 [1350000/3600000 (38%)]\tLoss: 0.450743\n","Train Epoch: 5 [1800000/3600000 (50%)]\tLoss: 0.443940\n","Train Epoch: 5 [2250000/3600000 (62%)]\tLoss: 0.446075\n","Train Epoch: 5 [2700000/3600000 (75%)]\tLoss: 0.442202\n","Train Epoch: 5 [3150000/3600000 (88%)]\tLoss: 0.444049\n","\n","Test set: Average loss: 0.4340, Accuracy: 719931/900000 (79.992%)\n","\n","Train Epoch: 6 [0/3600000 (0%)]\tLoss: 0.444766\n","Train Epoch: 6 [450000/3600000 (12%)]\tLoss: 0.443090\n","Train Epoch: 6 [900000/3600000 (25%)]\tLoss: 0.443560\n","Train Epoch: 6 [1350000/3600000 (38%)]\tLoss: 0.442910\n","Train Epoch: 6 [1800000/3600000 (50%)]\tLoss: 0.441084\n","Train Epoch: 6 [2250000/3600000 (62%)]\tLoss: 0.440938\n","Train Epoch: 6 [2700000/3600000 (75%)]\tLoss: 0.442857\n","Train Epoch: 6 [3150000/3600000 (88%)]\tLoss: 0.444740\n","\n","Test set: Average loss: 0.4336, Accuracy: 720061/900000 (80.007%)\n","\n","Train Epoch: 7 [0/3600000 (0%)]\tLoss: 0.443527\n","Train Epoch: 7 [450000/3600000 (12%)]\tLoss: 0.443332\n","Train Epoch: 7 [900000/3600000 (25%)]\tLoss: 0.444456\n","Train Epoch: 7 [1350000/3600000 (38%)]\tLoss: 0.441056\n","Train Epoch: 7 [1800000/3600000 (50%)]\tLoss: 0.439591\n","Train Epoch: 7 [2250000/3600000 (62%)]\tLoss: 0.441834\n","Train Epoch: 7 [2700000/3600000 (75%)]\tLoss: 0.444298\n","Train Epoch: 7 [3150000/3600000 (88%)]\tLoss: 0.438085\n","\n","Test set: Average loss: 0.4332, Accuracy: 720365/900000 (80.041%)\n","\n","Train Epoch: 8 [0/3600000 (0%)]\tLoss: 0.441358\n","Train Epoch: 8 [450000/3600000 (12%)]\tLoss: 0.442357\n","Train Epoch: 8 [900000/3600000 (25%)]\tLoss: 0.442458\n","Train Epoch: 8 [1350000/3600000 (38%)]\tLoss: 0.447384\n","Train Epoch: 8 [1800000/3600000 (50%)]\tLoss: 0.441066\n","Train Epoch: 8 [2250000/3600000 (62%)]\tLoss: 0.438104\n","Train Epoch: 8 [2700000/3600000 (75%)]\tLoss: 0.439570\n","Train Epoch: 8 [3150000/3600000 (88%)]\tLoss: 0.439132\n","\n","Test set: Average loss: 0.4330, Accuracy: 720374/900000 (80.042%)\n","\n","Train Epoch: 9 [0/3600000 (0%)]\tLoss: 0.444748\n","Train Epoch: 9 [450000/3600000 (12%)]\tLoss: 0.444011\n","Train Epoch: 9 [900000/3600000 (25%)]\tLoss: 0.440455\n","Train Epoch: 9 [1350000/3600000 (38%)]\tLoss: 0.447844\n","Train Epoch: 9 [1800000/3600000 (50%)]\tLoss: 0.442566\n","Train Epoch: 9 [2250000/3600000 (62%)]\tLoss: 0.436968\n","Train Epoch: 9 [2700000/3600000 (75%)]\tLoss: 0.438455\n","Train Epoch: 9 [3150000/3600000 (88%)]\tLoss: 0.443802\n","\n","Test set: Average loss: 0.4322, Accuracy: 720993/900000 (80.110%)\n","\n","Train Epoch: 10 [0/3600000 (0%)]\tLoss: 0.442964\n","Train Epoch: 10 [450000/3600000 (12%)]\tLoss: 0.442363\n","Train Epoch: 10 [900000/3600000 (25%)]\tLoss: 0.440949\n","Train Epoch: 10 [1350000/3600000 (38%)]\tLoss: 0.438531\n","Train Epoch: 10 [1800000/3600000 (50%)]\tLoss: 0.441405\n","Train Epoch: 10 [2250000/3600000 (62%)]\tLoss: 0.439945\n","Train Epoch: 10 [2700000/3600000 (75%)]\tLoss: 0.439147\n","Train Epoch: 10 [3150000/3600000 (88%)]\tLoss: 0.436102\n","\n","Test set: Average loss: 0.4318, Accuracy: 720894/900000 (80.099%)\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","args = parser.parse_args('--single_layer=True'.split())\n","\n","# set seed of random number generator\n","torch.manual_seed(args.seed)\n","\n","grid_search1(args)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5HqhpyubMeO_"},"source":["### 第二小问\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vL5o3sfJMY4X","outputId":"88c59570-7a0a-416b-c461-6358399e2f6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on 160000 examples\n","Using both high and low level features\n","Testing on 40000 examples\n","Using both high and low level features\n","\n"," training DNN with 1 layers. \n","\n","Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.828741\n","Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.528944\n","Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.507554\n","Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.488948\n","Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.481229\n","Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.479708\n","Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.469301\n","Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.462600\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bruce_Wong\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test set: Average loss: 0.4464, Accuracy: 31815/40000 (79.537%)\n","\n","Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.458606\n","Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.460160\n","Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.463563\n","Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.470462\n","Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.451369\n","Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.473385\n","Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.467879\n","Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.471058\n","\n","Test set: Average loss: 0.4398, Accuracy: 31929/40000 (79.823%)\n","\n","Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.465234\n","Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.454747\n","Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.449576\n","Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.446043\n","Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.464570\n","Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.438979\n","Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.459895\n","Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.462744\n","\n","Test set: Average loss: 0.4388, Accuracy: 31912/40000 (79.780%)\n","\n","Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.455452\n","Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.427595\n","Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.473334\n","Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.442161\n","Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.462574\n","Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.462375\n","Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.451717\n","Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.449895\n","\n","Test set: Average loss: 0.4368, Accuracy: 31942/40000 (79.855%)\n","\n","Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.465806\n","Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.427334\n","Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.444992\n","Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.454716\n","Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.446838\n","Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.438087\n","Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.448872\n","Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.480250\n","\n","Test set: Average loss: 0.4352, Accuracy: 31986/40000 (79.965%)\n","\n","Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.463054\n","Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.447379\n","Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.477403\n","Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.450234\n","Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.473961\n","Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.461966\n","Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.442346\n","Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.457301\n","\n","Test set: Average loss: 0.4361, Accuracy: 31958/40000 (79.895%)\n","\n","Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.433118\n","Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.444421\n","Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.456927\n","Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.452061\n","Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.461159\n","Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.442664\n","Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.462797\n","Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.454921\n","\n","Test set: Average loss: 0.4349, Accuracy: 31983/40000 (79.957%)\n","\n","Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.454857\n","Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.431256\n","Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.444295\n","Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.462882\n","Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.467976\n","Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.455258\n","Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.438072\n","Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.453941\n","\n","Test set: Average loss: 0.4345, Accuracy: 31974/40000 (79.935%)\n","\n","Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.454650\n","Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.446182\n","Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.449275\n","Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.441862\n","Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.451340\n","Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.446062\n","Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.428985\n","Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.459934\n","\n","Test set: Average loss: 0.4353, Accuracy: 31943/40000 (79.858%)\n","\n","Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.428013\n","Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.454760\n","Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.418575\n","Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.459437\n","Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.461942\n","Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.448135\n","Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.459370\n","Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.468391\n","\n","Test set: Average loss: 0.4350, Accuracy: 31975/40000 (79.938%)\n","\n","\n"," training DNN with 2 layers. \n","\n","Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.879931\n","Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.546984\n","Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.501750\n","Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.513172\n","Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.503172\n","Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.490733\n","Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.478816\n","Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.458105\n","\n","Test set: Average loss: 0.4479, Accuracy: 31833/40000 (79.582%)\n","\n","Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.471745\n","Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.464070\n","Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.469749\n","Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.483533\n","Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.470179\n","Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.476871\n","Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.476107\n","Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.471657\n","\n","Test set: Average loss: 0.4447, Accuracy: 31841/40000 (79.603%)\n","\n","Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.469964\n","Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.459280\n","Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.447093\n","Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.466266\n","Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.462722\n","Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.456065\n","Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.459001\n","Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.476470\n","\n","Test set: Average loss: 0.4382, Accuracy: 31929/40000 (79.823%)\n","\n","Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.458258\n","Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.466012\n","Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.433867\n","Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.451171\n","Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.449291\n","Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.449840\n","Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.489694\n","Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.445865\n","\n","Test set: Average loss: 0.4372, Accuracy: 31928/40000 (79.820%)\n","\n","Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.455541\n","Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.482234\n","Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.466635\n","Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.459469\n","Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.465950\n","Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.469420\n","Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.448689\n","Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.453513\n","\n","Test set: Average loss: 0.4353, Accuracy: 31993/40000 (79.983%)\n","\n","Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.445209\n","Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.458836\n","Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.450567\n","Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.439850\n","Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.437869\n","Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.453135\n","Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.447151\n","Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.452987\n","\n","Test set: Average loss: 0.4386, Accuracy: 31977/40000 (79.942%)\n","\n","Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.459567\n","Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.479812\n","Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.450910\n","Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.446527\n","Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.424162\n","Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.460225\n","Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.470196\n","Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.433821\n","\n","Test set: Average loss: 0.4351, Accuracy: 31991/40000 (79.978%)\n","\n","Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.447511\n","Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.445915\n","Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.450458\n","Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.473075\n","Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.467105\n","Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.430189\n","Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.474303\n","Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.453193\n","\n","Test set: Average loss: 0.4357, Accuracy: 32021/40000 (80.052%)\n","\n","Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.450812\n","Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.475487\n","Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.425631\n","Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.464629\n","Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.460335\n","Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.462403\n","Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.433854\n","Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.442289\n","\n","Test set: Average loss: 0.4341, Accuracy: 31987/40000 (79.968%)\n","\n","Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.465057\n","Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.456546\n","Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.455897\n","Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.478803\n","Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.443612\n","Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.435820\n","Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.440432\n","Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.437724\n","\n","Test set: Average loss: 0.4346, Accuracy: 31948/40000 (79.870%)\n","\n","\n"," training DNN with 3 layers. \n","\n","Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.770237\n","Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.583303\n","Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.534363\n","Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.488098\n","Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.496058\n","Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.494857\n","Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.514784\n","Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.499754\n","\n","Test set: Average loss: 0.5210, Accuracy: 29580/40000 (73.950%)\n","\n","Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.476230\n","Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.466586\n","Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.455697\n","Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.473810\n","Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.459458\n","Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.475764\n","Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.486870\n","Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.457452\n","\n","Test set: Average loss: 0.5377, Accuracy: 29157/40000 (72.892%)\n","\n","Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.456646\n","Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.465906\n","Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.456295\n","Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.461707\n","Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.469617\n","Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.465549\n","Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.462027\n","Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.449527\n","\n","Test set: Average loss: 0.5083, Accuracy: 30086/40000 (75.215%)\n","\n","Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.479508\n","Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.466580\n","Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.452731\n","Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.473061\n","Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.432165\n","Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.440267\n","Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.457224\n","Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.442321\n","\n","Test set: Average loss: 0.5206, Accuracy: 29772/40000 (74.430%)\n","\n","Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.463144\n","Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.451734\n","Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.485254\n","Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.449494\n","Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.442168\n","Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.456510\n","Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.466608\n","Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.460028\n","\n","Test set: Average loss: 0.5001, Accuracy: 30432/40000 (76.080%)\n","\n","Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.454811\n","Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.462185\n","Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.468519\n","Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.443221\n","Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.438289\n","Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.467472\n","Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.469228\n","Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.455758\n","\n","Test set: Average loss: 0.4705, Accuracy: 31405/40000 (78.513%)\n","\n","Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.454409\n","Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.464977\n","Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.448412\n","Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.466693\n","Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.497421\n","Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.456112\n","Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.446463\n","Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.461360\n","\n","Test set: Average loss: 0.4810, Accuracy: 30883/40000 (77.207%)\n","\n","Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.451166\n","Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.455629\n","Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.446818\n","Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.468496\n","Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.450123\n","Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.460205\n","Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.473256\n","Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.449171\n","\n","Test set: Average loss: 0.4927, Accuracy: 30618/40000 (76.545%)\n","\n","Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.449684\n","Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.459641\n","Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.444586\n","Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.452272\n","Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.472565\n","Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.444040\n","Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.459670\n","Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.439715\n","\n","Test set: Average loss: 0.4629, Accuracy: 31601/40000 (79.002%)\n","\n","Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.460002\n","Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.436428\n","Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.440709\n","Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.446418\n","Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.452862\n","Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.450307\n","Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.456139\n","Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.466376\n","\n","Test set: Average loss: 0.4760, Accuracy: 31316/40000 (78.290%)\n","\n","\n"," training DNN with 4 layers. \n","\n","Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.780076\n","Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.619700\n","Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.545839\n","Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.526121\n","Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.511205\n","Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.525131\n","Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.508387\n","Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.488195\n","\n","Test set: Average loss: 0.6437, Accuracy: 26799/40000 (66.998%)\n","\n","Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.494277\n","Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.486366\n","Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.484142\n","Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.502602\n","Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.473768\n","Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.481551\n","Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.458078\n","Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.469546\n","\n","Test set: Average loss: 0.6008, Accuracy: 27570/40000 (68.925%)\n","\n","Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.461704\n","Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.464118\n","Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.464434\n","Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.477144\n","Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.484595\n","Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.473404\n","Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.471418\n","Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.458849\n","\n","Test set: Average loss: 0.5439, Accuracy: 29129/40000 (72.823%)\n","\n","Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.476221\n","Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.480313\n","Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.482942\n","Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.468408\n","Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.466895\n","Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.458953\n","Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.468013\n","Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.471462\n","\n","Test set: Average loss: 0.5562, Accuracy: 28588/40000 (71.470%)\n","\n","Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.474924\n","Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.467666\n","Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.447034\n","Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.485556\n","Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.464177\n","Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.449952\n","Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.466648\n","Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.466616\n","\n","Test set: Average loss: 0.5586, Accuracy: 28758/40000 (71.895%)\n","\n","Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.472423\n","Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.452052\n","Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.467821\n","Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.474934\n","Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.452849\n","Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.472118\n","Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.466962\n","Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.464376\n","\n","Test set: Average loss: 0.5283, Accuracy: 29573/40000 (73.933%)\n","\n","Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.464191\n","Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.469500\n","Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.467678\n","Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.438880\n","Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.453336\n","Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.473848\n","Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.472799\n","Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.441309\n","\n","Test set: Average loss: 0.5359, Accuracy: 29121/40000 (72.802%)\n","\n","Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.457513\n","Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.467442\n","Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.430116\n","Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.443787\n","Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.451354\n","Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.459507\n","Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.480333\n","Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.457431\n","\n","Test set: Average loss: 0.5031, Accuracy: 30245/40000 (75.612%)\n","\n","Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.486363\n","Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.458199\n","Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.455623\n","Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.459772\n","Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.465659\n","Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.453397\n","Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.449003\n","Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.457876\n","\n","Test set: Average loss: 0.4959, Accuracy: 30667/40000 (76.668%)\n","\n","Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.462218\n","Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.474836\n","Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.445340\n","Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.467464\n","Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.454774\n","Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.460218\n","Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.442836\n","Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.446721\n","\n","Test set: Average loss: 0.5071, Accuracy: 30307/40000 (75.767%)\n","\n","\n"," training DNN with 5 layers. \n","\n","Train Epoch: 1 [0/160000 (0%)]\tLoss: 0.839040\n","Train Epoch: 1 [20000/160000 (12%)]\tLoss: 0.677318\n","Train Epoch: 1 [40000/160000 (25%)]\tLoss: 0.624309\n","Train Epoch: 1 [60000/160000 (38%)]\tLoss: 0.570279\n","Train Epoch: 1 [80000/160000 (50%)]\tLoss: 0.550210\n","Train Epoch: 1 [100000/160000 (62%)]\tLoss: 0.515530\n","Train Epoch: 1 [120000/160000 (75%)]\tLoss: 0.511238\n","Train Epoch: 1 [140000/160000 (88%)]\tLoss: 0.484912\n","\n","Test set: Average loss: 0.6884, Accuracy: 24832/40000 (62.080%)\n","\n","Train Epoch: 2 [0/160000 (0%)]\tLoss: 0.521069\n","Train Epoch: 2 [20000/160000 (12%)]\tLoss: 0.483790\n","Train Epoch: 2 [40000/160000 (25%)]\tLoss: 0.502495\n","Train Epoch: 2 [60000/160000 (38%)]\tLoss: 0.494121\n","Train Epoch: 2 [80000/160000 (50%)]\tLoss: 0.489806\n","Train Epoch: 2 [100000/160000 (62%)]\tLoss: 0.505078\n","Train Epoch: 2 [120000/160000 (75%)]\tLoss: 0.480560\n","Train Epoch: 2 [140000/160000 (88%)]\tLoss: 0.480625\n","\n","Test set: Average loss: 0.6710, Accuracy: 24453/40000 (61.133%)\n","\n","Train Epoch: 3 [0/160000 (0%)]\tLoss: 0.489869\n","Train Epoch: 3 [20000/160000 (12%)]\tLoss: 0.463446\n","Train Epoch: 3 [40000/160000 (25%)]\tLoss: 0.494086\n","Train Epoch: 3 [60000/160000 (38%)]\tLoss: 0.482344\n","Train Epoch: 3 [80000/160000 (50%)]\tLoss: 0.470365\n","Train Epoch: 3 [100000/160000 (62%)]\tLoss: 0.458490\n","Train Epoch: 3 [120000/160000 (75%)]\tLoss: 0.472425\n","Train Epoch: 3 [140000/160000 (88%)]\tLoss: 0.489850\n","\n","Test set: Average loss: 0.6116, Accuracy: 25892/40000 (64.730%)\n","\n","Train Epoch: 4 [0/160000 (0%)]\tLoss: 0.469055\n","Train Epoch: 4 [20000/160000 (12%)]\tLoss: 0.468373\n","Train Epoch: 4 [40000/160000 (25%)]\tLoss: 0.483876\n","Train Epoch: 4 [60000/160000 (38%)]\tLoss: 0.464069\n","Train Epoch: 4 [80000/160000 (50%)]\tLoss: 0.472185\n","Train Epoch: 4 [100000/160000 (62%)]\tLoss: 0.489143\n","Train Epoch: 4 [120000/160000 (75%)]\tLoss: 0.443427\n","Train Epoch: 4 [140000/160000 (88%)]\tLoss: 0.472935\n","\n","Test set: Average loss: 0.5942, Accuracy: 26352/40000 (65.880%)\n","\n","Train Epoch: 5 [0/160000 (0%)]\tLoss: 0.457920\n","Train Epoch: 5 [20000/160000 (12%)]\tLoss: 0.464473\n","Train Epoch: 5 [40000/160000 (25%)]\tLoss: 0.460096\n","Train Epoch: 5 [60000/160000 (38%)]\tLoss: 0.490159\n","Train Epoch: 5 [80000/160000 (50%)]\tLoss: 0.464016\n","Train Epoch: 5 [100000/160000 (62%)]\tLoss: 0.452212\n","Train Epoch: 5 [120000/160000 (75%)]\tLoss: 0.454418\n","Train Epoch: 5 [140000/160000 (88%)]\tLoss: 0.462904\n","\n","Test set: Average loss: 0.5602, Accuracy: 27463/40000 (68.657%)\n","\n","Train Epoch: 6 [0/160000 (0%)]\tLoss: 0.457374\n","Train Epoch: 6 [20000/160000 (12%)]\tLoss: 0.474086\n","Train Epoch: 6 [40000/160000 (25%)]\tLoss: 0.456221\n","Train Epoch: 6 [60000/160000 (38%)]\tLoss: 0.469433\n","Train Epoch: 6 [80000/160000 (50%)]\tLoss: 0.460536\n","Train Epoch: 6 [100000/160000 (62%)]\tLoss: 0.479338\n","Train Epoch: 6 [120000/160000 (75%)]\tLoss: 0.478792\n","Train Epoch: 6 [140000/160000 (88%)]\tLoss: 0.477461\n","\n","Test set: Average loss: 0.5214, Accuracy: 29143/40000 (72.858%)\n","\n","Train Epoch: 7 [0/160000 (0%)]\tLoss: 0.450832\n","Train Epoch: 7 [20000/160000 (12%)]\tLoss: 0.454352\n","Train Epoch: 7 [40000/160000 (25%)]\tLoss: 0.479825\n","Train Epoch: 7 [60000/160000 (38%)]\tLoss: 0.468026\n","Train Epoch: 7 [80000/160000 (50%)]\tLoss: 0.480247\n","Train Epoch: 7 [100000/160000 (62%)]\tLoss: 0.461305\n","Train Epoch: 7 [120000/160000 (75%)]\tLoss: 0.474242\n","Train Epoch: 7 [140000/160000 (88%)]\tLoss: 0.466995\n","\n","Test set: Average loss: 0.5171, Accuracy: 29588/40000 (73.970%)\n","\n","Train Epoch: 8 [0/160000 (0%)]\tLoss: 0.473885\n","Train Epoch: 8 [20000/160000 (12%)]\tLoss: 0.461010\n","Train Epoch: 8 [40000/160000 (25%)]\tLoss: 0.456671\n","Train Epoch: 8 [60000/160000 (38%)]\tLoss: 0.431126\n","Train Epoch: 8 [80000/160000 (50%)]\tLoss: 0.456704\n","Train Epoch: 8 [100000/160000 (62%)]\tLoss: 0.454724\n","Train Epoch: 8 [120000/160000 (75%)]\tLoss: 0.438807\n","Train Epoch: 8 [140000/160000 (88%)]\tLoss: 0.460964\n","\n","Test set: Average loss: 0.4913, Accuracy: 30645/40000 (76.612%)\n","\n","Train Epoch: 9 [0/160000 (0%)]\tLoss: 0.456006\n","Train Epoch: 9 [20000/160000 (12%)]\tLoss: 0.439069\n","Train Epoch: 9 [40000/160000 (25%)]\tLoss: 0.462040\n","Train Epoch: 9 [60000/160000 (38%)]\tLoss: 0.454644\n","Train Epoch: 9 [80000/160000 (50%)]\tLoss: 0.460235\n","Train Epoch: 9 [100000/160000 (62%)]\tLoss: 0.468345\n","Train Epoch: 9 [120000/160000 (75%)]\tLoss: 0.446604\n","Train Epoch: 9 [140000/160000 (88%)]\tLoss: 0.462484\n","\n","Test set: Average loss: 0.5190, Accuracy: 29496/40000 (73.740%)\n","\n","Train Epoch: 10 [0/160000 (0%)]\tLoss: 0.452159\n","Train Epoch: 10 [20000/160000 (12%)]\tLoss: 0.457337\n","Train Epoch: 10 [40000/160000 (25%)]\tLoss: 0.456573\n","Train Epoch: 10 [60000/160000 (38%)]\tLoss: 0.459347\n","Train Epoch: 10 [80000/160000 (50%)]\tLoss: 0.472579\n","Train Epoch: 10 [100000/160000 (62%)]\tLoss: 0.456800\n","Train Epoch: 10 [120000/160000 (75%)]\tLoss: 0.473002\n","Train Epoch: 10 [140000/160000 (88%)]\tLoss: 0.449331\n","\n","Test set: Average loss: 0.5007, Accuracy: 30409/40000 (76.022%)\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["args = parser.parse_args()\n","\n","# set seed of random number generator\n","torch.manual_seed(args.seed)\n","\n","grid_search2(args)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNz+CXKpzj0HtNr9j0QfWDw","mount_file_id":"1Fjhd3kTkLUdf6t-xIERAQBqw7Sykrn44","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"f9ac8dd3a99bb1ea99ca7c2b25771441ad4391538e000abd3dfba9abc5b655bd"}}},"nbformat":4,"nbformat_minor":0}
